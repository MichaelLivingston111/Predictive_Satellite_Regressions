---
title: "Satellite_Sensor_Model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Upload required libraries:
```{r}

suppressMessages(library(ggplot2))
suppressMessages(library(caret))
suppressMessages(library(ggpubr))
suppressMessages(library(MuMIn))
suppressMessages(library(lme4))
suppressMessages(library(feather))
suppressMessages(library(arrow))
suppressMessages(library(dplyr))
suppressMessages(library(viridis))
suppressMessages(library(grid))
suppressMessages(library(car))
suppressMessages(library(GGally))

```


# Upload all the data, and sort into workable dataframes:
```{r}

# Upload the data from a total data file:
total_data <- read.csv("Total_Sat_Data_All_Var.csv")

# Specify all the relevant variables:
data <- total_data %>% select(TEP, Temperature, Season, Log_Chl, POC, Avg_PAR)

```


# Step-wise Regressions models: Both forward and backward sequences for both types of models:
```{r}

# Define intercept-only model
intercept_only <- lm(TEP ~ 1, data = data)  # linear
intercept_lmer <- lmer(TEP ~ 1 + (1|Season), data = data)  # mixed effects

# Define model with all predictors
all <- lm(TEP ~ ., data = data)  # linear
all_lmer <- lmer(TEP ~ . + (1|Season), data = data)  # mixed effects

```


# Perform forward stepwise regression:
```{r}
forward <- step(intercept_only, direction='forward', scope=formula(all), trace=0)
forward$anova
forward$coefficients

plot(forward$residuals ~ forward$fitted.values)

```


# Perform backward stepwise regression
```{r}

# Perform backward stepwise regression
backward <- step(all, direction ='backward', scope = formula(all), trace=0)
backward$anova
backward$coefficients

plot(backward$residuals ~ backward$fitted.values)


# Both model selection techniques coverged on the same set of variables, outlined below.
```



# Specify ideal mixed effects and linear models based on the variable selections above:
```{r}

# Ideal satellite model:
Satellite_All_lmer = lmer(TEP ~ Temperature + Log_Chl + POC + (1|Season), data = data)

summary(Satellite_All_lmer)
r.squaredGLMM(Satellite_All_lmer)


# Linear model:
Satellite_lm = lm(TEP ~ Temperature + Log_Chl + POC + Season, data = data)

summary(Satellite_lm)
r.squaredGLMM(Satellite_lm)


# Base satellite model:
Satellite_lm_base = lm(TEP ~ Temperature + Log_Chl + Season, data = data)

summary(Satellite_lm_base)
r.squaredGLMM(Satellite_lm_base)

```


# May need to eliminate variables to avoid multicollinearity:
```{r}

car::vif(Satellite_All_lmer)
car::vif(Satellite_lm)
car::vif(Satellite_lm_base)

# No concerns about multicollinearity....

```


# What model is prefferred? Check AIC and AICc:
```{r}

AIC(Satellite_All_lmer, Satellite_lm, Satellite_lm_base)
AICc(Satellite_All_lmer, Satellite_lm, Satellite_lm_base)

# The mixed effects models is clearly preferred. 

```


# Checking model assumptions: Equal variance and normal distribution of model residuals:
```{r}


# Get the model residuals:
data$resid1 <- resid(Satellite_lmer)  
data$predict1 <- predict(Satellite_lmer) 

data$resid2 <- resid(Satellite_lm)  
data$predict2 <- predict(Satellite_lm) 

data$resid3 <- resid(Satellite_lmer_base)  
data$predict3 <- predict(Satellite_lm_base) 




# Residual, Q-Q, Cooks distance plots:
plot(Satellite_lmer)  # Residuals
plot(Satellite_lm)
plot(Satellite_lmer_base)

# QQ plots:
Q_1 <- qqPlot(data$predict1, pch = 1)
Q_2 <- qqPlot(data$predict2, pch = 1)
Q_3 <- qqPlot(data$predict3, pch = 1)

# Plot the distribution of the linear model residuals:
H_model1 <- ggplot(data = data, aes(resid1)) + 
  geom_histogram(binwidth = 0.5, colour = "black", fill = "white") +
  xlab("Residuals") +
  ylab("Count") +
  theme_pubr()

H_model2 <- ggplot(data = data, aes(resid2)) + 
  geom_histogram(binwidth = 0.5, colour = "black", fill = "white") +
  xlab("Residuals") +
  ylab("Count") +
  theme_pubr()

H_model3 <- ggplot(data = data, aes(resid3)) + 
  geom_histogram(binwidth = 0.5, colour = "black", fill = "white") +
  xlab("Residuals") +
  ylab("Count") +
  theme_pubr()


ggarrange(H_model1, H_model2, H_model3)




# Use the Shapiro-Wilks test to test for normality distributions in the residuals:
shapiro.test(data$resid1) 
shapiro.test(data$resid2) 
shapiro.test(data$resid3) 

```


# Perfrom a 25x cross validation to assess the accuracy of these models on new data. 
```{r}

# Containers for the predictions and accuracy measurements:

sample_train <- NULL

# Forward selections:
Sat_lmer_predict <- NULL
Sat_lmer_RMSE <- NULL
Sat_lmer_MAE <- NULL

Sat_lm_predict <- NULL
Sat_lm_RMSE <- NULL
Sat_lm_MAE <- NULL

Sat_lm_base_predict <- NULL
Sat_lm_base_RMSE <- NULL
Sat_lm_base_MAE <- NULL


# Loop through model prediction and perform 25x cross validation. 
# The model is trained on 95% of the data, and validated on the other 5%

for (i in 1:30) {
  
  #Creating a re-sampled data set from the total data:
  training.samples <- createDataPartition(data$TEP, p = 0.95, list = FALSE)
  train.data1  <- data[training.samples, ]  # Training set
  test.data1 <- data[-training.samples, ]  # Testing set
  
  
  # Running the models and creating predictions, accuracy metrics on the partitioned training data:
  
  # Mixed effects:
  Sat_lmer_CV<- lmer(TEP ~ Log_Chl + Temperature + POC + (1|Season), data = train.data1) 

  # Linear model:
  Sat_lm_CV<- lm(TEP ~ Log_Chl + Temperature + POC, data = train.data1) 
  
  # Base model:
  Sat_lm_base_CV<- lm(TEP ~ Log_Chl + Temperature + Season, data = train.data1) 
  
  
  # Model 'true' training points, applies to each model:
  sample_train <- c(sample_train, test.data1$TEP)
  
  
  
  # Model predictions and error calculations:
  
  # Mixed effects:
  Sat_lmer_predict <- c(Sat_lmer_predict, predict(Sat_lmer_CV, test.data1, 
                                                type = "response", allow.new.levels = TRUE))  # Predictions
  
  Sat_lmer_RMSE <- c(Sat_lmer_RMSE, RMSE(Sat_lmer_predict, sample_train))  # RMSE
  
  Sat_lmer_MAE <- c(Sat_lmer_MAE, MAE(Sat_lmer_predict, sample_train))  # RMSE
  

  # Linear model:
  Sat_lm_predict <- c(Sat_lm_predict, predict(Sat_lm_CV, test.data1, 
                                                type = "response", allow.new.levels = TRUE))  # Predictions
  
  Sat_lm_RMSE <- c(Sat_lm_RMSE, RMSE(Sat_lm_predict, sample_train))  # RMSE
  
  Sat_lm_MAE <- c(Sat_lm_MAE, MAE(Sat_lm_predict, sample_train))  # RMSE
  
  
  # Base model:
  Sat_lm_base_predict <- c(Sat_lm_base_predict, predict(Sat_lm_base_CV, test.data1, 
                                                type = "response", allow.new.levels = TRUE))  # Predictions
  
  Sat_lm_base_RMSE <- c(Sat_lm_base_RMSE, RMSE(Sat_lm_base_predict, sample_train))  # RMSE
  
  Sat_lm_base_MAE <- c(Sat_lm_base_MAE, MAE(Sat_lm_base_predict, sample_train))  # RMSE
}


# The above loop generated 25 different regression models with model coefficients, y intercepts and predictions, specified below for each of the models created: 

predictions <- cbind(Sat_lmer_predict, Sat_lm_predict, Sat_lm_base_predict)
predictions <- as.data.frame(predictions)  # as a data frame

```


# Plot the results of the cross validation:
```{r}

Figure <- ggplot(predictions, aes(x = sample_train)) +
    #geom_point(aes(y = Sat_lmer_predict), alpha = 0.6, colour = "blue") +
    geom_point(aes(y = Sat_lm_predict), alpha = 0.6, colour = "dodgerblue") +
    #geom_point(aes(y = Sat_lm_base_predict), alpha = 0.6, colour = "black") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    ggtitle("") +
    ylim(0, 175) +
    xlim(0, 175) +
    ylab("") +
    xlab("") +
    theme_pubr() +
    theme(plot.title=element_text(size=8,face="bold"))


Figure

```


# Compare all the model accuracies with mean absolutew errors:
```{r}


# Get the MAE:
MAE <- cbind(Sat_lmer_MAE, Sat_lm_MAE, Sat_lm_base_MAE)
MAE <- as.data.frame(MAE)

MAE <- colMeans(MAE)
print(MAE)

MAE_boxplot <- boxplot(Sat_lmer_MAE, Sat_lm_MAE, Sat_lm_base_MAE, main='MAE',
        names=c('Mixed effects','Linear', 'Base Linear'), 
        outline=FALSE, ylim=c(7,15))


# glmnet
```