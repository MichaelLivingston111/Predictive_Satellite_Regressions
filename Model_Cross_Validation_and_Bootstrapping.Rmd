---
title: "Mixed_effects_model_validation_and_bootsrapping"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Upload required libraries:
```{r}

suppressMessages(library(ggplot2))
suppressMessages(library(caret))
suppressMessages(library(ggpubr))
suppressMessages(library(MuMIn))
suppressMessages(library(lme4))
suppressMessages(library(feather))
suppressMessages(library(arrow))
suppressMessages(library(dplyr))

```


# Upload all the data, and sort into workable dataframes:
```{r}

# Upload the data from a total data file:
total_data <- read.csv("Total_Sat_Data_All_Var.csv")
# total_data <- read.csv("Total_Sat_Data.csv")
# plot_data <- read.csv("Total_Sat_Data2.csv")

UpperMixed_data <- total_data

```

# CREATE THE REGRESSION ALGORITHM: Linear mixed effects model
```{r}

# Create the linear mixed effects model:
model1 <- lmer(TEP ~ Log_Chl + Temperature + (1|Season), data = UpperMixed_data)

summary(model1)  # Get a model summary

# The random effects section tells us how much variance we find among levels of our grouping factors, plus residual variance.
# Here we find that the differences between seasons explains ~30% of the total variation, or the left over variation after the variance explained by the fixed effects.

# Obtain R2 values:
r.squaredGLMM(model1)


# Marginal R_GLMM² represents the variance explained by the fixed effects
# Conditional R_GLMM² is interpreted as a variance explained by the entire model, including both fixed and random effects

```


# Checking model assumptions: Equal variance and normal distribution of model residuals:
```{r}

# Get the linear model residuals:
UpperMixed_data$resid1 <- resid(model1)  
UpperMixed_data$predict1 <- predict(model1) 

# Residual plot:
plot(model1 )  # Residuals
Res_model1 <- ggplot(data = UpperMixed_data, aes(predict1, resid1)) + 
  geom_point() + 
  xlab("Fitted Values") +
  ylab("Model Residuals") +
  theme_pubr()
Res_model1

# Plot the distribution of the linear model residuals:
H_model1 <- ggplot(data = UpperMixed_data, aes(resid1)) + 
  geom_histogram(binwidth = 1, colour = "black", fill = "white") +
  xlab("Residuals") +
  ylab("Count") +
  theme_pubr()
H_model1

# Plot a qqplot to visualize normality:
qqnorm(resid(model1))
qqline(resid(model1))

# Use the Shapiro-Wilks test to test for normality distributions in the lm residuals:
shapiro.test(UpperMixed_data$resid1) 

```


# Bootstrapping model cross validation. Here, I will create 1000 random mixed effects models from random combinations of data in the dataset, make predictions and view the results.

```{r}

# Containers for the coefficients, predictions and accuracy measurements:
sample_coef_intercept <- NULL
sample_coef_x1 <- NULL
sample_predict <- NULL
sample_predict_2 <- NULL
sample_train <- NULL
RMSE <- NULL
MAE <- NULL

# Loop through model prediciton and cross validation 500x. The model is trained on 95% of the data, and validated on the other 5%, 500x.

for (i in 1:50) {
  
  #Creating a re-sampled data set from the total data:
  training.samples <- createDataPartition(UpperMixed_data$TEP, p = 0.95, list = FALSE)
  train.data1  <- UpperMixed_data[training.samples, ]  # Training set
  test.data1 <- UpperMixed_data[-training.samples, ]  # Testing set
  
  # Running the model on the partitioned training data:
  model_bootstrap <- lmer(TEP ~ Log_Chl + Temperature + (1|Season), data = train.data1)
  model_bootstrap_2 <- lm(TEP ~ Log_Chl + Nitrate + DO + Season, data = train.data1)
  
  # Model predictions:
  sample_predict <- 
    c(sample_predict, predict(model_bootstrap, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  sample_predict_2 <- 
    c(sample_predict_2, predict(model_bootstrap_2, test.data1, type = "response", 
                              allow.new.levels = TRUE))

  # Model 'true' validation points:
  sample_train <- c(sample_train, test.data1$TEP)
  
  # RMSE:
  RMSE <- c(RMSE, RMSE(sample_predict, sample_train))
  
  # MAE:
  MAE <- c(MAE, MAE(sample_predict, sample_train))
}

# The above loop generated 1000 different regression models with model coefficients, y intercepts and predictions, specified below: 
predictions <- cbind(sample_predict, sample_predict_2, sample_train)
predictions <- as.data.frame(predictions)  # as a data frame
RMSE <- as.data.frame(RMSE)
MAE <- as.data.frame(MAE)

# Plot the bootstrapping cross validation:
ggplot(predictions, aes(sample_train)) +
  geom_point(aes(y = sample_predict), alpha = 0.6) +
  geom_point(aes(y = sample_predict_2), color = 'red', alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +  # Add a 1:1 line
  # geom_smooth(method = 'lm') +
  ylab("Predicted TEP concentration") +
  xlab("Measured TEP concentration") +
  xlim(0, 175) +
  ylim(0, 175) +
  theme_classic()

# Obtain the Root Mean Square Error and Mean Absolute Error for the linear models - this will give us an estimate of their accuracy.

mean_total <- mean(UpperMixed_data$TEP)  # Calculate mean of true values

Avg_RMSE = lapply(RMSE, mean)
Avg_RMSE = as.numeric(Avg_RMSE)

Avg_MAE = lapply(MAE, mean)
Avg_MAE = as.numeric(Avg_MAE)

print(paste("The RMSE of model1 predictions is", Avg_RMSE))
print(paste("The RMSE % of the mean for model1 is", 
            (Avg_RMSE/mean_total) * 100))

print(paste("The MAE of model1 predictions is", Avg_MAE))
print(paste("The MAE % of the mean for model1 is", 
            (Avg_MAE/mean_total) * 100))


```

# Create a series of models and cross validate each one over 100x in order to compare the predicitive ability of each model.
```{r}

# Containers for the coefficients, predictions and accuracy measurements:
sample_coef_intercept <- NULL
sample_coef_x1 <- NULL
sample_predict <- NULL
sample_predict_2 <- NULL
sample_predict_3 <- NULL
sample_predict_4 <- NULL
sample_train <- NULL
RMSE <- NULL
MAE <- NULL
RMSE_2 <- NULL
MAE_2 <- NULL
RMSE_3 <- NULL
MAE_3 <- NULL
RMSE_4 <- NULL
MAE_4 <- NULL

# Loop through model prediciton and cross validation 500x. The model is trained on 95% of the data, and validated on the other 5%, 500x.

for (i in 1:50) {
  
  #Creating a re-sampled data set from the total data:
  training.samples <- createDataPartition(UpperMixed_data$TEP, p = 0.95, list = FALSE)
  train.data1  <- UpperMixed_data[training.samples, ]  # Training set
  test.data1 <- UpperMixed_data[-training.samples, ]  # Testing set
  
  # Running the model on the partitioned training data:
  model_bootstrap <- lmer(TEP ~ Log_Chl + Temperature + (1|Season), data = train.data1)  # Satellite model
  model_bootstrap_2 <- lm(TEP ~ Log_Chl, data = train.data1)
  model_bootstrap_3 <- lm(TEP ~ Log_Chl + Nitrate + DO + Season, data = train.data1)  # Ideal model
  model_bootstrap_4 <- lm(TEP ~ MLD + DO, data = train.data1)
  
  # Model 'true' training points, applies to each model:
  sample_train <- c(sample_train, test.data1$TEP)
  
  
  # Model predictions and error calculations:
  
  # Satellite model:
  sample_predict <- 
    c(sample_predict, predict(model_bootstrap, test.data1, type = "response", 
                              allow.new.levels = TRUE))
    
  RMSE <- c(RMSE, RMSE(sample_predict, sample_train))  # RMSE:
  MAE <- c(MAE, MAE(sample_predict, sample_train))  # MAE:
  
  # Model bootstrap 2:
  sample_predict_2 <- 
    c(sample_predict_2, predict(model_bootstrap_2, test.data1, type = "response", 
                                allow.new.levels = TRUE))
  
  RMSE_2 <- c(RMSE_2, RMSE(sample_predict_2, sample_train))  # RMSE:
  MAE_2 <- c(MAE_2, MAE(sample_predict_2, sample_train))  # MAE:
  
  
  # Model bootstrap 3/ideal model:
  sample_predict_3 <- 
    c(sample_predict_3, predict(model_bootstrap_3, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  
  RMSE_3 <- c(RMSE_3, RMSE(sample_predict_3, sample_train))  # RMSE:
  MAE_3 <- c(MAE_3, MAE(sample_predict_3, sample_train))  # MAE:
  
  
  # Model bootstrap 4:
  sample_predict_4 <- 
    c(sample_predict_4, predict(model_bootstrap_4, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  
  RMSE_4 <- c(RMSE_4, RMSE(sample_predict_4, sample_train))  # RMSE:
  MAE_4 <- c(MAE_4, MAE(sample_predict_4, sample_train))  # MAE:
  
}

# The above loop generated 1000 different regression models with model coefficients, y intercepts and predictions, specified below for each of the models created: 
predictions <- cbind(sample_predict, sample_predict_2, sample_predict_3, sample_predict_4, sample_train)
predictions <- as.data.frame(predictions)  # as a data frame

```


# Obtain the Root Mean Square Error and Mean Absolute Error for the linear models - this will give us an estimate of their accuracy.
```{r}

# Create data frames for the accuracy measurements:
RMSE <- cbind(RMSE, RMSE_2, RMSE_3, RMSE_4)
RMSE <- as.data.frame(RMSE)

MAE <- cbind(MAE, MAE_2, MAE_3, MAE_4)
MAE <- as.data.frame(MAE)

mean_total <- mean(UpperMixed_data$TEP)  # Calculate mean of true values

# Get the average MAE and RMSE for each model:

MAE <- colMeans(MAE)
RMSE <- colMeans(RMSE)

print(MAE)

```


# Plot the results:
```{r}

colors = c ("Chl + Temp + (1|Season)" = "red", 
            "Chl" = "blue", 
            "Chl + DO + Nitrate + Season" = "purple", 
            "MLD + DO" = "gray")


# Plot the bootstrapping cross validation:
ggplot(predictions, aes(sample_train)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +  # Add a 1:1 line
  
  geom_smooth(method = 'lm', aes(y = sample_predict, color = "Chl + Temp + (1|Season)"), se = FALSE, lty = 1) +
  
  geom_smooth(method = 'lm', aes(y = sample_predict_2, color = "Chl"), se = FALSE, lty = 2) +
  
  geom_smooth(method = 'lm', aes(y = sample_predict_3,color = "Chl + DO + Nitrate + Season"), se = FALSE, lty = 3) +
  
  geom_smooth(method = 'lm', aes(y = sample_predict_4,color = "MLD + DO"), se = FALSE, lty = 4) +
  
  labs(x = "Measured TEP", y = "Predicted TEP", color = "Legend") +
  
  scale_color_manual(values = colors) + 
  
  xlim(0, 150) +
  ylim(0, 150) +
  theme_classic()


```


