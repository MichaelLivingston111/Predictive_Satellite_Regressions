---
title: "Mixed_effects_model_validation_and_bootsrapping"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Upload required libraries:
```{r}

suppressMessages(library(ggplot2))
suppressMessages(library(caret))
suppressMessages(library(ggpubr))
suppressMessages(library(MuMIn))
suppressMessages(library(lme4))
suppressMessages(library(feather))
suppressMessages(library(arrow))
suppressMessages(library(dplyr))
suppressMessages(library(viridis))

```


# Upload all the data, and sort into workable dataframes:
```{r}

# Upload the data from a total data file:
total_data <- read.csv("Total_Sat_Data_All_Var.csv")
# total_data <- read.csv("Total_Sat_Data.csv")
# plot_data <- read.csv("Total_Sat_Data2.csv")

UpperMixed_data <- total_data

```


# Plot the relationship between the predictor variable and main response variable, and visualize the relationship.

```{r}

Log_model <- lm(TEP ~ Log_Chl, data = total_data)

ggplot(total_data, aes(Chl, TEP, colour = Nitrate)) + 
  geom_point() +
  scale_color_viridis(option = "viridis") + 
  stat_smooth(method = lm, formula = y ~ log(x), se = FALSE)

ggplot(total_data, aes(Log_Chl, TEP, colour = Nitrate) ) + 
  geom_point() +
  scale_color_viridis(option = "H") + 
  stat_smooth(method = lm, se = FALSE, lty = 'dashed', color = 'black') + 
  xlab("") +
  ylab("") +
  theme(legend.title = element_blank()) +
  theme_classic()

ggplot(total_data, aes(Log_MLD, TEP, colour = MLD) ) + 
  geom_point() +
  scale_color_viridis(option = "H") + 
  stat_smooth(method = lm, se = FALSE, lty = 'dashed', color = 'black') + 
  xlab("") +
  ylab("") +
  theme(legend.title = element_blank()) +
  theme_classic()

```



# CREATE THE REGRESSION ALGORITHM: Linear mixed effects model
```{r}

# Create the linear mixed effects model:
model1 <- lmer(TEP ~ Log_Chl + Temperature + (1|Season), data = UpperMixed_data)

model1 <- lm(TEP ~ Log_Chl + Log_MLD + DO + Nitrate, data = UpperMixed_data)

summary(model1)  # Get a model summary

# The random effects section tells us how much variance we find among levels of our grouping factors, plus residual variance.
# Here we find that the differences between seasons explains ~30% of the total variation, or the left over variation after the variance explained by the fixed effects.

# Obtain R2 values:
r.squaredGLMM(model1)


# Marginal R_GLMM² represents the variance explained by the fixed effects
# Conditional R_GLMM² is interpreted as a variance explained by the entire model, including both fixed and random effects

```


# Checking model assumptions: Equal variance and normal distribution of model residuals:
```{r}

# Get the linear model residuals:
UpperMixed_data$resid1 <- resid(model1)  
UpperMixed_data$predict1 <- predict(model1) 

# Residual plot:
plot(model1 )  # Residuals
Res_model1 <- ggplot(data = UpperMixed_data, aes(predict1, resid1)) + 
  geom_point() + 
  xlab("Fitted Values") +
  ylab("Model Residuals") +
  theme_pubr()
Res_model1

# Plot the distribution of the linear model residuals:
H_model1 <- ggplot(data = UpperMixed_data, aes(resid1)) + 
  geom_histogram(binwidth = 1, colour = "black", fill = "white") +
  xlab("Residuals") +
  ylab("Count") +
  theme_pubr()
H_model1

# Plot a qqplot to visualize normality:
qqnorm(resid(model1))
qqline(resid(model1))

# Use the Shapiro-Wilks test to test for normality distributions in the lm residuals:
shapiro.test(UpperMixed_data$resid1) 

```


# Create a series of models and cross validate each one over 100x in order to compare the predicitive ability of each model.
```{r}

# Containers for the coefficients, predictions and accuracy measurements:
sample_coef_intercept <- NULL
sample_coef_x1 <- NULL
sample_predict <- NULL
sample_predict_2 <- NULL
sample_predict_3 <- NULL
sample_predict_4 <- NULL
sample_predict_5 <- NULL
sample_predict_6 <- NULL
sample_predict_7 <- NULL
sample_predict_8 <- NULL
sample_train <- NULL
RMSE <- NULL
MAE <- NULL
RMSE_2 <- NULL
MAE_2 <- NULL
RMSE_3 <- NULL
MAE_3 <- NULL
RMSE_4 <- NULL
MAE_4 <- NULL
RMSE_5 <- NULL
MAE_5 <- NULL
RMSE_6 <- NULL
MAE_6 <- NULL
RMSE_7 <- NULL
MAE_7 <- NULL
RMSE_8 <- NULL
MAE_8 <- NULL




# Loop through model prediciton and cross validation 500x. The model is trained on 95% of the data, and validated on the other 5%, 500x.

for (i in 1:50) {
  
  #Creating a re-sampled data set from the total data:
  training.samples <- createDataPartition(UpperMixed_data$TEP, p = 0.95, list = FALSE)
  train.data1  <- UpperMixed_data[training.samples, ]  # Training set
  test.data1 <- UpperMixed_data[-training.samples, ]  # Testing set
  
  # Running the model on the partitioned training data:
  
  model_bootstrap <- lm(TEP ~ Log_Chl, data = train.data1) 
  model_bootstrap_2 <- lm(TEP ~ Log_Chl + Nitrate, data = train.data1)
  model_bootstrap_3 <- lm(TEP ~ Log_Chl + Nitrate + DO + factor(Season), data = train.data1)
  model_bootstrap_4 <- lm(TEP ~ MLD + Nitrate + DO + factor(Season), data = train.data1)
  model_bootstrap_5 <- lmer(TEP ~ Log_Chl + Temperature + (1|Season), data = train.data1) 
  model_bootstrap_6 <- lmer(TEP ~ Log_Chl + Nitrate + DO + (1|Season), data = train.data1)
  model_bootstrap_7 <- lmer(TEP ~ Log_Chl + Log_MLD + (1|Season), data = train.data1) 
  model_bootstrap_8 <- lmer(TEP ~ Log_Chl + Log_MLD + Temperature + (1|Season), data = train.data1)
  
  # Model 'true' training points, applies to each model:
  sample_train <- c(sample_train, test.data1$TEP)
  
  
  # Model predictions and error calculations:
  
  # 1
  sample_predict <- 
    c(sample_predict, predict(model_bootstrap, test.data1, type = "response", 
                              allow.new.levels = TRUE))
    
  RMSE <- c(RMSE, RMSE(sample_predict, sample_train))  # RMSE:
  MAE <- c(MAE, MAE(sample_predict, sample_train))  # MAE:
  
  # 2
  sample_predict_2 <- 
    c(sample_predict_2, predict(model_bootstrap_2, test.data1, type = "response", 
                                allow.new.levels = TRUE))
  
  RMSE_2 <- c(RMSE_2, RMSE(sample_predict_2, sample_train))  # RMSE:
  MAE_2 <- c(MAE_2, MAE(sample_predict_2, sample_train))  # MAE:
  
  
  # 3
  sample_predict_3 <- 
    c(sample_predict_3, predict(model_bootstrap_3, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  
  RMSE_3 <- c(RMSE_3, RMSE(sample_predict_3, sample_train))  # RMSE:
  MAE_3 <- c(MAE_3, MAE(sample_predict_3, sample_train))  # MAE:
  
  
  # 4
  sample_predict_4 <- 
    c(sample_predict_4, predict(model_bootstrap_4, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  
  RMSE_4 <- c(RMSE_4, RMSE(sample_predict_4, sample_train))  # RMSE:
  MAE_4 <- c(MAE_4, MAE(sample_predict_4, sample_train))  # MAE:
  
  # 5
  sample_predict_5 <- 
    c(sample_predict_5, predict(model_bootstrap_5, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  
  RMSE_5 <- c(RMSE_5, RMSE(sample_predict_5, sample_train))  # RMSE:
  MAE_5 <- c(MAE_5, MAE(sample_predict_5, sample_train))  # MAE:
  
  # 6
  sample_predict_6 <- 
    c(sample_predict_6, predict(model_bootstrap_6, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  
  RMSE_6 <- c(RMSE_6, RMSE(sample_predict_6, sample_train))  # RMSE:
  MAE_6 <- c(MAE_5, MAE(sample_predict_6, sample_train))  # MAE:
  
  # 7
  sample_predict_7 <- 
    c(sample_predict_7, predict(model_bootstrap_7, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  
  RMSE_7 <- c(RMSE_7, RMSE(sample_predict_7, sample_train))  # RMSE:
  MAE_7 <- c(MAE_7, MAE(sample_predict_7, sample_train))  # MAE:
  
  # 8
  sample_predict_8 <- 
    c(sample_predict_8, predict(model_bootstrap_8, test.data1, type = "response", 
                              allow.new.levels = TRUE))
  
  RMSE_8 <- c(RMSE_8, RMSE(sample_predict_8, sample_train))  # RMSE:
  MAE_8 <- c(MAE_8, MAE(sample_predict_8, sample_train))  # MAE:
  
}

# The above loop generated 1000 different regression models with model coefficients, y intercepts and predictions, specified below for each of the models created: 
predictions <- cbind(sample_predict, sample_predict_2, sample_predict_3, sample_predict_4, sample_predict_5, sample_predict_6, sample_predict_7, sample_predict_8, sample_train)
predictions <- as.data.frame(predictions)  # as a data frame

```


# Plot the results of the cross validations:
```{r}

# Create a function to subset the predictions dataframe and returns a plot:

# Create the workable dataframe:
# Create example data frame
plot_df <- data.frame(
                     Model1 = sample_predict,
                     Model2 = sample_predict_2,
                     Model3 = sample_predict_3,
                     Model4 = sample_predict_4,
                     Model5 = sample_predict_5,
                     Model6 = sample_predict_6,
                     Model7 = sample_predict_7,
                     Model8 = sample_predict_8,
                     train_data = sample_train)

Model1 = cbind(predictions = sample_predict, sample_train)
Model2 = cbind(predictions = sample_predict_2, sample_train)
Model3 = cbind(predictions = sample_predict_3, sample_train)
Model4 = cbind(predictions = sample_predict_4, sample_train)
Model5 = cbind(predictions = sample_predict_5, sample_train)
Model6 = cbind(predictions = sample_predict_6, sample_train)
Model7 = cbind(predictions = sample_predict_7, sample_train)
Model8 = cbind(predictions = sample_predict_8, sample_train)

list = c("Model1", "Model2", "Model3", "Model4", "Model5", "Model6", "Model7", "Model8")

# Design a function
myplot <- function(data, title){
  ggplot(data, aes(x = sample_train, y = predictions)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = 'lm') + 
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    ylim(0, 160) +
    xlim(0, 160) +
    theme_classic() +
    labs(title = title)
}

for(i in list){
  print(myplot(get(i), i))
}

```


# Obtain the Root Mean Square Error and Mean Absolute Error for the linear models - this will give us an estimate of their accuracy.
```{r}

# Create data frames for the accuracy measurements:
RMSE <- cbind(RMSE, RMSE_2, RMSE_3, RMSE_4, RMSE_5, RMSE_6)
RMSE <- as.data.frame(RMSE)

MAE <- cbind(MAE, MAE_2, MAE_3, MAE_4, MAE_5, MAE_6)
MAE <- as.data.frame(MAE)

mean_total <- mean(UpperMixed_data$TEP)  # Calculate mean of true values

# Get the average MAE and RMSE for each model:

MAE <- colMeans(MAE)
RMSE <- colMeans(RMSE)

print(MAE)
print(RMSE)

```


# Plot the results:
```{r}

colors = c ("Chl" = "red", 
            "Chl + Nitrate" = "blue", 
            "MLD + DO + Nitrate" = "gray",
            "Chl + Nitrate + DO + factor(Season)" = "purple",
            "Chl + Temp + (1|Season)" = "darkgreen",
            "Chl + DO + Nitrate + (1|Season)" = "magenta"
            )


# Plot the bootstrapping cross validation:
ggplot(predictions, aes(sample_train)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +  # Add a 1:1 line
  
  geom_smooth(method = 'lm', aes(y = sample_predict, 
                                 color = "Chl"), se = FALSE, lty = 1) +
  
  geom_smooth(method = 'lm', aes(y = sample_predict_2, 
                                 color = "Chl + Nitrate"), se = FALSE, lty = 2) +
  
  geom_smooth(method = 'lm', aes(y = sample_predict_3, 
                                 color = "Chl + Nitrate + DO + factor(Season)"), se = FALSE, lty = 3) +
  
  geom_smooth(method = 'lm', aes(y = sample_predict_4, 
                                 color = "MLD + DO + Nitrate"), se = FALSE, lty = 4) +
  
  geom_smooth(method = 'lm', aes(y = sample_predict_5, 
                                 color = "Chl + Temp + (1|Season)"), se = FALSE, lty = 5) +

  geom_smooth(method = 'lm', aes(y = sample_predict_6, 
                                 color = "Chl + DO + Nitrate + (1|Season)"), se = FALSE, lty = 5) +
  
  labs(x = "Measured TEP", y = "Predicted TEP", color = "Legend") +
  
  scale_color_manual(values = colors) + 
  
  xlim(0, 150) +
  ylim(0, 150) +
  theme_classic()


```

# Compare all models with AIC and AICc values, and the R2:
```{r}

# Recreate the same models outside of the loop:
model1 <- lm(TEP ~ Log_Chl, data = train.data1) 
model2 <- lm(TEP ~ Log_Chl + Nitrate, data = train.data1)
model3 <- lm(TEP ~ Log_Chl + Nitrate + DO + factor(Season), data = train.data1)
model4<- lm(TEP ~ MLD + Nitrate + DO + factor(Season), data = train.data1)
model5 <- lmer(TEP ~ Log_Chl + Temperature + (1|Season), data = train.data1) 
model6 <- lmer(TEP ~ Log_Chl + Nitrate + DO + (1|Season), data = train.data1)

AICc(model1, model2, model3, model4, model5, model6)
AIC(model1, model2, model3, model4, model5, model6)

# Obtain R2 values:
M1_R2 <- r.squaredGLMM(model1)
M2_R2 <- r.squaredGLMM(model2)
M3_R2 <- r.squaredGLMM(model3)
M4_R2 <- r.squaredGLMM(model4)
M5_R2 <- r.squaredGLMM(model5)
M6_R2 <- r.squaredGLMM(model6)

# Make a table of the values:
df <- data.frame(M1_R2, M2_R2, M3_R2, M4_R2, M5_R2, M6_R2)
colnames(df) <- c('Model1 R2m', 'Model1 R2c', 'Model2 R2m', 'Model2 R2c',
                  'Model3 R2m', 'Model3 R2c', 'Model4 R2m', 'Model4 R2c',
                  'Model5 R2m', 'Model5 R2c', 'Model6 R2m', 'Model6 R2c')
print(df)


```
